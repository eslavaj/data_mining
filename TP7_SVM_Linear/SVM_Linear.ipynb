{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://jeslava-pc:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1585679542909)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// mkdir data\n",
    "// wget -nc http://cedric.cnam.fr/vertigo/Cours/RCP216/docs/a7.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                  |\n",
      "+-----+----------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(123,[2,10,13,18,38,41,54,63,66,72,74,75,79,82],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0.0  |(123,[2,5,16,21,35,40,52,63,66,72,73,75,79,82],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "+-----+----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "donnees: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Lire les données dans le format approprié\n",
    "val donnees = spark.read.format(\"libsvm\").load(\"file:///home/jeslava/msata/cnam_RCP216/data_mining/TP7_SVM_Linear/data/a7\")\n",
    "donnees.show(2, false)  // visualiser les 2 premières lignes du DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitions: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])\n",
       "apprentissage: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Partitionner les données en apprentissage (50%) et test (50%)\n",
    "val partitions = donnees.randomSplit(Array(0.5, 0.5))\n",
    "val apprentissage = partitions(0).cache()  // conservé en mémoire\n",
    "val test = partitions(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LinearSVC\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "linSvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_2628fa455300\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_d146b9079f16\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Construction de pipeline\n",
    "import org.apache.spark.ml.classification.LinearSVC\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "// Définition de l'estimateur SVC linéaire\n",
    "val linSvc = new LinearSVC().setMaxIter(10)\n",
    "                            .setFeaturesCol(\"features\")\n",
    "                            .setLabelCol(\"label\")\n",
    "\n",
    "// Définition du pipeline (réduit ici au SVM linéaire)\n",
    "val pipeline = new Pipeline().setStages(Array(linSvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_2628fa455300-regParam: 0.02\n",
       "}, {\n",
       "\tlinearsvc_2628fa455300-regParam: 0.05\n",
       "}, {\n",
       "\tlinearsvc_2628fa455300-regParam: 0.1\n",
       "}, {\n",
       "\tlinearsvc_2628fa455300-regParam: 0.2\n",
       "}, {\n",
       "\tlinearsvc_2628fa455300-regParam: 0.3\n",
       "}, {\n",
       "\tlinearsvc_2628fa455300-regParam: 0.4\n",
       "}, {\n",
       "\tlinearsvc_2628fa455300-regParam: 0.5\n",
       "})\n",
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_bd970a1cacd2\n",
       "cvSVCmodel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_bd970a1cacd2\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "// Construction de la grille de (hyper)paramètres utilisée pour grid search\n",
    "// Une composante est ajoutée avec .addGrid() pour chaque (hyper)paramètre à explorer\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "                           .addGrid(linSvc.regParam,\n",
    "                                    Array(0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5))\n",
    "                           .build()\n",
    "\n",
    "// Définition de l'instance de CrossValidator : à quel estimateur l'appliquer,\n",
    "//  avec quels (hyper)paramètres, combien de folds, comment évaluer les résultats\n",
    "val cv = new CrossValidator().setEstimator(pipeline)\n",
    "                             .setEstimatorParamMaps(paramGrid)\n",
    "                             .setNumFolds(5)\n",
    "                             .setEvaluator(new BinaryClassificationEvaluator())\n",
    "\n",
    "// Construction et évaluation par validation croisée des modèles correspondant\n",
    "//   à toutes les combinaisons de valeurs de (hyper)paramètres de paramGrid\n",
    "val cvSVCmodel = cv.fit(apprentissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[Double] = Array(0.8937221590149094, 0.8919098051750863, 0.893076203632652, 0.8923139510919373, 0.8916698228699008, 0.8916453922738434, 0.8910546328127511)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Afficher les valeurs AUC obtenues pour les combinaisons de la grille\n",
    "cvSVCmodel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.ml.param.ParamMap =\n",
       "{\n",
       "\tlinearsvc_2628fa455300-regParam: 0.02\n",
       "}\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Afficher les meilleures valeurs pour les hyperparamètres\n",
    "cvSVCmodel.getEstimatorParamMaps.zip(cvSVCmodel.avgMetrics).maxBy(_._2)._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Enregistrement du meilleur modèle\n",
    "cvSVCmodel.write.save(\"file:///home/jeslava/msata/cnam_RCP216/data_mining/TP7_SVM_Linear/cvSVCmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+-----+------------------------------------------+----------+\n",
      "|label|rawPrediction                             |prediction|\n",
      "+-----+------------------------------------------+----------+\n",
      "|0.0  |[0.7921919119369204,-0.7921919119369204]  |0.0       |\n",
      "|0.0  |[-0.17729742321150285,0.17729742321150285]|1.0       |\n",
      "|0.0  |[0.38845192144805984,-0.38845192144805984]|0.0       |\n",
      "|0.0  |[0.5091415603678989,-0.5091415603678989]  |0.0       |\n",
      "|0.0  |[1.2869319403234856,-1.2869319403234856]  |0.0       |\n",
      "|0.0  |[1.1878277439932219,-1.1878277439932219]  |0.0       |\n",
      "|0.0  |[1.784685660686986,-1.784685660686986]    |0.0       |\n",
      "|0.0  |[1.784685660686986,-1.784685660686986]    |0.0       |\n",
      "|0.0  |[0.8810793402457736,-0.8810793402457736]  |0.0       |\n",
      "|0.0  |[0.8810793402457736,-0.8810793402457736]  |0.0       |\n",
      "+-----+------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "resApp: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 2 more fields]\n",
       "resultats: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Évaluation du modèle obtenu avec les valeurs optimales des hyper-paramètres\n",
    "// Calculer les prédictions sur les données d'apprentissage\n",
    "val resApp = cvSVCmodel.transform(apprentissage)\n",
    "\n",
    "// Calculer les prédictions sur les données de test\n",
    "val resultats = cvSVCmodel.transform(test)\n",
    "\n",
    "// Afficher le schéma du DataFrame resultats\n",
    "resultats.printSchema()\n",
    "\n",
    "// Afficher 10 lignes complètes de résultats (sans la colonne features)\n",
    "resultats.select(\"label\", \"rawPrediction\", \"prediction\").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8949034137954631\n",
      "0.8873289977592711\n"
     ]
    }
   ],
   "source": [
    "// Calculer AUC sur données d'apprentissage\n",
    "println(cvSVCmodel.getEvaluator.evaluate(resApp))\n",
    "\n",
    "// Calculer AUC sur données de test\n",
    "println(cvSVCmodel.getEvaluator.evaluate(resultats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
