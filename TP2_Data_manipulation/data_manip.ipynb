{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://wifi-auditeur-367.cnam.fr:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1582053403005)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectDense: org.apache.spark.ml.linalg.Vector = [1.5,0.0,3.5]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*Create a vector dense */\n",
    "val vectDense = Vectors.dense(1.5, 0.0, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectZeros: org.apache.spark.ml.linalg.Vector = [0.0,0.0,0.0]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*Create vector dense with all components to zero*/\n",
    "val vectZeros = Vectors.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectCreux1: org.apache.spark.ml.linalg.Vector = (3,[0,2],[1.5,3.5])\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**/\n",
    "val vectCreux1 = Vectors.sparse(3, Array(0, 2), Array(1.5, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectCreux2: org.apache.spark.ml.linalg.Vector = (3,[0,2],[1.5,3.5])\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**/\n",
    "val vectCreux2 = Vectors.sparse( 3, Seq( (0, 1.5), (2, 3.5) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,[0,2],[1.5,3.5])\n"
     ]
    }
   ],
   "source": [
    "println(vectCreux1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,[0,2],[1.5,3.5])\n"
     ]
    }
   ],
   "source": [
    "println(vectCreux2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "println(vectCreux1.equals(vectCreux2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "println(vectCreux1.equals(vectDense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    }
   ],
   "source": [
    "println(vectCreux1.equals(vectZeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "println(vectDense.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "println(vectZeros.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "println(vectCreux1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "println(vectCreux2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectCreux3: org.apache.spark.ml.linalg.Vector = (3,[0,2],[1.5,3.5])\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*Deep copy*/\n",
    "val vectCreux3 = vectCreux1.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Double = 5.0\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*L1 norm*/\n",
    "Vectors.norm(vectDense, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Double = 5.0\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.norm(vectCreux1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Double = 3.8078865529319543\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*L2 norm*/\n",
    "Vectors.norm(vectDense, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Double = 3.8078865529319543\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.norm(vectCreux1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Double = 14.5\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*Squared distance between 2 vectors */\n",
    "Vectors.sqdist(vectDense, vectZeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Double = 0.0\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.sqdist(vectDense, vectCreux1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.{Matrix, DenseMatrix, SparseMatrix}\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Matrix, DenseMatrix, SparseMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matDense: org.apache.spark.ml.linalg.DenseMatrix =\n",
       "1.2  2.3\n",
       "3.4  4.5\n",
       "5.6  6.7\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Create a dense matrix ((1.2, 2.3), (3.4, 4.5), (5.6, 6.7)) */\n",
    "val matDense = new DenseMatrix(3, 2, Array(1.2, 3.4, 5.6, 2.3, 4.5, 6.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vect3: org.apache.spark.ml.linalg.Vector = [1.0,2.0]\n",
       "matProd: org.apache.spark.ml.linalg.DenseVector = [5.8,12.4,19.0]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vect3 = Vectors.dense(1.0, 2.0)\n",
    "val matProd = matDense.multiply(vect3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.8,12.4,19.0]\n"
     ]
    }
   ],
   "source": [
    "println(matProd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "donnees: Array[org.apache.spark.ml.linalg.Vector] = Array((3,[0],[1.0]), [0.0,1.0,0.0], [0.0,0.0,1.0])\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*We will build a dataset/dataframe of small size only for testing*/\n",
    "val donnees = Array(Vectors.sparse(3, Seq((0, 1.0))), Vectors.dense(0.0, 1.0, 0.0), Vectors.dense(0.0, 0.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "donneesdf: org.apache.spark.sql.DataFrame = [vecteurs: vector]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val donneesdf = spark.createDataFrame(donnees.map(Tuple1.apply)).toDF(\"vecteurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|     vecteurs|\n",
      "+-------------+\n",
      "|(3,[0],[1.0])|\n",
      "|[0.0,1.0,0.0]|\n",
      "|[0.0,0.0,1.0]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "donneesdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geysercsvdf: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Dataframe by using a csv file\n",
    "val geysercsvdf = spark.read.format(\"csv\").load(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geyser.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geysercsvdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  _c0|   _c1|\n",
      "+-----+------+\n",
      "|3.600|79.000|\n",
      "|1.800|54.000|\n",
      "|3.333|74.000|\n",
      "|2.283|62.000|\n",
      "|4.533|85.000|\n",
      "+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geysercsvdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geysercsvdf2: org.apache.spark.sql.DataFrame = [column1: string, column2: string]\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geysercsvdf2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geyser2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geysercsvdf3: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geysercsvdf3 = spark.read.format(\"csv\").option(\"sep\", \" \").load(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geyser.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  _c0|   _c1|\n",
      "+-----+------+\n",
      "|3.600|79.000|\n",
      "|1.800|54.000|\n",
      "|3.333|74.000|\n",
      "|2.283|62.000|\n",
      "|4.533|85.000|\n",
      "+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geysercsvdf3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geysercsvdf3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.43.232:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1582657757324)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/*Using format LIBSVM*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "libsvmdf: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create dataframe from LIBSVM file\n",
    "val libsvmdf = spark.read.format(\"libsvm\").load(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res6: Array[org.apache.spark.sql.Row] = Array([0.0,(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,2..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// display the dataframe\n",
    "libsvmdf.printSchema()\n",
    "libsvmdf.show(5)\n",
    "libsvmdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                100|\n",
      "|   mean|               0.57|\n",
      "| stddev|0.49756985195624287|\n",
      "|    min|                0.0|\n",
      "|    max|                1.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Calculate stats for the column « label »\n",
    "libsvmdf.describe(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.rdd.RDD\n",
       "lignes: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[52] at map at MLUtils.scala:205\n",
       "res10: Array[org.apache.spark.mllib.linalg.Vector] = Array([3.6,79.0])\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.rdd.RDD\n",
    "//Get a RDD\n",
    "val lignes = MLUtils.loadVectors(sc, \"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geysers.txt\")\n",
    "lignes.count()\n",
    "lignes.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "chaineSchema: String = val1 val2\n",
       "champs: Array[org.apache.spark.sql.types.StructField] = Array(StructField(val1,DoubleType,true), StructField(val2,DoubleType,true))\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(val1,DoubleType,true), StructField(val2,DoubleType,true))\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[53] at map at <console>:42\n",
       "lignesdf: org.apache.spark.sql.DataFrame = [val1: double, val2: double]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now we will use this RDD to obtain a Dataframe\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Define the scheme du Dataframe as a char string\n",
    "val chaineSchema = \"val1 val2\"\n",
    "\n",
    "// Build the scheme du Dataframe as a char string\n",
    "val champs = chaineSchema.split(\" \").map(nomChamp => StructField(nomChamp, DoubleType, true))\n",
    "val schema = StructType(champs)\n",
    "\n",
    "// Build a RDD[Row] from RDD[Vector]\n",
    "val rowRDD = lignes.map(ligne => Row(ligne(0),ligne(1)))\n",
    "\n",
    "// Build DataFrame from RDD[Row]\n",
    "val lignesdf = spark.createDataFrame(rowRDD, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- val1: double (nullable = true)\n",
      " |-- val2: double (nullable = true)\n",
      "\n",
      "+-----+----+\n",
      "| val1|val2|\n",
      "+-----+----+\n",
      "|  3.6|79.0|\n",
      "|  1.8|54.0|\n",
      "|3.333|74.0|\n",
      "|2.283|62.0|\n",
      "|4.533|85.0|\n",
      "+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// See obtained Dataframe\n",
    "lignesdf.printSchema()\n",
    "lignesdf.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              val1|\n",
      "+-------+------------------+\n",
      "|  count|               272|\n",
      "|   mean|3.4877830882352945|\n",
      "| stddev|1.1413712511052085|\n",
      "|    min|               1.6|\n",
      "|    max|               5.1|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|              val2|\n",
      "+-------+------------------+\n",
      "|  count|               272|\n",
      "|   mean|  70.8970588235294|\n",
      "| stddev|13.594973789999397|\n",
      "|    min|              43.0|\n",
      "|    max|              96.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// Calculate stats\n",
    "lignesdf.describe(\"val1\").show()\n",
    "lignesdf.describe(\"val2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vectors\n",
       "donnees: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geyser.txt MapPartitionsRDD[70] at textFile at <console>:39\n",
       "lignes2: org.apache.spark.rdd.RDD[org.apache.spark.ml.linalg.Vector] = MapPartitionsRDD[71] at map at <console>:40\n",
       "res13: Long = 272\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Now we will obtain a Dataframe via a RDD from \n",
    "// geyser.txt (this file is not adapted to loadVectors function)\n",
    "// To do this we will proceed as follows\n",
    "\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val donnees = sc.textFile(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geyser.txt\")\n",
    "val lignes2 = donnees.map(s => Vectors.dense(s.split(' ').map(_.toDouble)))\n",
    "lignes2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- val1: double (nullable = true)\n",
      " |-- val2: double (nullable = true)\n",
      "\n",
      "+-----+----+\n",
      "| val1|val2|\n",
      "+-----+----+\n",
      "|  3.6|79.0|\n",
      "|  1.8|54.0|\n",
      "|3.333|74.0|\n",
      "|2.283|62.0|\n",
      "|4.533|85.0|\n",
      "+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "geysercsvdfd: org.apache.spark.sql.DataFrame = [val1: double, val2: double]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Specify the format of colomns\n",
    "val geysercsvdfd = spark.read.format(\"csv\").schema(schema).load(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geyser.csv\")\n",
    "geysercsvdfd.printSchema()\n",
    "geysercsvdfd.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|              val1|              val2|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               272|               272|\n",
      "|   mean|3.4877830882352936|  70.8970588235294|\n",
      "| stddev|1.1413712511052083|13.594973789999392|\n",
      "|    min|               1.6|              43.0|\n",
      "|    max|               5.1|              96.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// calc stats for all numeric columns \n",
    "geysercsvdfd.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      "\n",
      "+-----+----+\n",
      "|  _c0| _c1|\n",
      "+-----+----+\n",
      "|  3.6|79.0|\n",
      "|  1.8|54.0|\n",
      "|3.333|74.0|\n",
      "|2.283|62.0|\n",
      "|4.533|85.0|\n",
      "+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testdfd: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The same result can be obtained with DataFrameReader.option(\"inferSchema\",true)\n",
    "val testdfd = spark.read.format(\"csv\").option(\"inferSchema\",true).load(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP2_Data_manipulation/data/geyser.csv\")\n",
    "testdfd.printSchema()\n",
    "testdfd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
