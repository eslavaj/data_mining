{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages = [\"org.vegas-viz:vegas_2.11:0.3.11\", \"org.vegas-viz:vegas-spark_2.11:0.3.11\"]\n",
    "launcher.master = \"local[4]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://jeslava-pc:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[4], app id = local-1585077084153)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.streaming._\n",
       "import org.apache.spark.mllib.linalg.Vectors\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.clustering.StreamingKMeans\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.clustering.StreamingKMeans\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@fe8ded9\n",
       "trainingData: org.apache.spark.streaming.dstream.DStream[org.apache.spark.mllib.linalg.Vector] = org.apache.spark.streaming.dstream.MappedDStream@5d56ff94\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Création du StreamingContext : le flux sera découpés en \"tranches\" de 10 secondes,\n",
    "//   chaque tranche sera un RDD\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "\n",
    "// Lecture du flux de données d'entree : les nouveaux fichiers déposés dans\n",
    "//   \"data/stream\" seront rassemblés dans un RDD toutes les 10s et traités\n",
    "val trainingData = ssc.textFileStream(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP6_dataflow/stream\").map(Vectors.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numDimensions: Int = 2\n",
       "numClusters: Int = 5\n",
       "model: org.apache.spark.mllib.clustering.StreamingKMeans = org.apache.spark.mllib.clustering.StreamingKMeans@3302292a\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Paramétrage de Streaming k-means\n",
    "val numDimensions = 2   // données 2D\n",
    "val numClusters = 5     // recherche de 5 groupes\n",
    "val model = new StreamingKMeans()  // création d'un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: model.type = org.apache.spark.mllib.clustering.StreamingKMeans@3302292a\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Initialisation des paramètres du modèle\n",
    "model.setK(numClusters)\n",
    "model.setDecayFactor(0.5)   // valeur de alpha\n",
    "model.setRandomCenters(numDimensions, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Indication du flux sur lequel le modèle est appris (les groupes sont trouvés)\n",
    "model.trainOn(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultats: org.apache.spark.streaming.dstream.DStream[Int] = org.apache.spark.streaming.dstream.MappedDStream@1434d8cb\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Indication du flux sur lequel les prédictions sont faites par le modèle\n",
    "val resultats = model.predictOn(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// À titre pédagogique pour la visualisation, on sauvegarde sur le disque\n",
    "// les RDD des points traités et des clusters prédits\n",
    "trainingData.foreachRDD(rdd => rdd.saveAsTextFile(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP6_dataflow/points\"))\n",
    "resultats.foreachRDD(rdd => rdd.saveAsTextFile(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP6_dataflow/resultats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Launch spark flow\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "4: error: ';' expected but ',' found.",
     "output_type": "error",
     "traceback": [
      "<console>:4: error: ';' expected but ',' found.",
      "cp data/full/{aaa,aab,aac} stream/",
      "                 ^",
      ""
     ]
    }
   ],
   "source": [
    "// Launch the following command from a terminal:\n",
    "// cp data/full/{aaa,aab,aac} stream/\n",
    "//And check results on resultats folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cluster: integer (nullable = false)\n",
      " |-- coords: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clusters: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[608] at map at <console>:67\n",
       "points: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[611] at map at <console>:68\n",
       "predictions: org.apache.spark.sql.DataFrame = [cluster: int, coords: array<double>]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// On lit les RDD et on créé un DataFrame à deux colonnes\n",
    "val clusters = sc.textFile(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP6_dataflow/resultats/\").map(_.toInt)\n",
    "val points = sc.textFile(\"file:///home/hadoop/msata/cnam_RCP216/data_mining/TP6_dataflow/points\").map(p => Vectors.parse(p).toArray)\n",
    "val predictions = clusters.zip(points).toDF(\"cluster\", \"coords\")\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "render: vegas.render.ShowHTML = <function1>\n",
       "import vegas._\n",
       "import vegas.data.External._\n",
       "import vegas.sparkExt._\n",
       "scatter: org.apache.spark.sql.DataFrame = [cluster: int, coords: array<double> ... 2 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vegas permet de visualiser les clusters ainsi obtenus\n",
    "// Import des bibliothèques de Vegas\n",
    "implicit val render = vegas.render.ShowHTML(s => print(\"%html \" + s))\n",
    "\n",
    "import vegas._\n",
    "import vegas.data.External._\n",
    "import vegas.sparkExt._\n",
    "\n",
    "// Construction du nuage de points\n",
    "val scatter = predictions.withColumn(\"x\", $\"coords\".getItem(0)).withColumn(\"y\", $\"coords\".getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vegas(\"Données initiales\").withDataFrame(scatter).mark(Point).encodeX(\"x\", Quant).encodeY(\"y\", Quant).encodeColor(\"cluster\", Nom).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[org.apache.spark.mllib.linalg.Vector] = Array([0.22567614752825793,0.5023218135489114], [0.22567614752827792,0.5023218135489314], [-0.44362107723763694,-0.6120968293161826], [-0.18663023693410216,0.7351128192338361], [-0.4436210772376569,-0.6120968293162026])\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Les valeurs des centres des clusters sont accessibles via l’objet lastModel() qui \n",
    "// renvoie le dernier modèle KMeans calculé \n",
    "//à partir des données du flux\n",
    "model.latestModel().clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now we are going to copy all files \n",
    "// cp data/full/* stream/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scatter: org.apache.spark.sql.DataFrame = [cluster: int, coords: array<double> ... 2 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now we visualize again \n",
    "// Construction du nuage de points\n",
    "val scatter = predictions.withColumn(\"x\", $\"coords\".getItem(0)).withColumn(\"y\", $\"coords\".getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vegas(\"Données initiales\").withDataFrame(scatter).mark(Point).encodeX(\"x\", Quant).encodeY(\"y\", Quant).encodeColor(\"cluster\", Nom).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[org.apache.spark.mllib.linalg.Vector] = Array([0.14152378740134425,0.34215244153016555], [0.5722041751206708,0.4592565578824435], [-0.4293069658063142,-0.5686628757709866], [-0.0941069953719659,0.7432846122300463], [-0.44941142835276404,-0.6783933093799789])\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// And new centers are:\n",
    "model.latestModel().clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
